# ROBOTS.txt

> Un peu comme la dynamite, ce fichier doit √™tre manipul√© avec beaucoup de pr√©caution.
> Il permet de donner des instructions aux robots d'indexation des moteurs de recherche (Google, Bing, etc.) sur les pages qu'ils peuvent ou ne peuvent pas indexer.
> Il est crucial de bien comprendre son fonctionnement pour √©viter de bloquer accidentellement l'acc√®s √† des parties importantes de votre site.
> Il est recommand√© de tester soigneusement les r√®gles d√©finies dans ce fichier avant de le mettre en production.
>
> *Note :* Ce fichier doit √™tre plac√© √† la racine de votre site web pour √™tre pris en compte par les robots d'indexation.

## Structure du fichier

Il y a deux fa√ßons de proc√©der :

- utiliser un plugin (YOAST).
- le cr√©er manuellement.

### Methode plugin, YOAST SEO

De nombreux plugins SEO pour les CMS populaires (comme WordPress, Drupal, Joomla, etc.) offrent une interface conviviale pour g√©rer le fichier `robots.txt` sans avoir √† le cr√©er manuellement.
Par exemple, le plugin Yoast SEO pour WordPress permet de g√©n√©rer et de modifier facilement le fichier `robots.txt` directement depuis le tableau de bord de WordPress. Il suffit d'aller dans la section "Outils" du plugin, puis de s√©lectionner "√âditeur de fichiers" pour acc√©der au fichier `robots.txt` et le modifier selon vos besoins.

### Methode manuelle

Pour cr√©er un fichier `robots.txt` manuellement, suivez ces √©tapes :

1. Ouvrez un √©diteur de texte (comme Notepad, VSCode, Sublime Text, etc.).
2. Ajoutez les directives souhait√©es (voir la section suivante pour les principales directives).
3. Enregistrez le fichier sous le nom `robots.txt`.
4. T√©l√©chargez le fichier √† la racine de votre site web (par exemple, `https://www.votresite.com/robots.txt`).
5. Testez le fichier en utilisant des outils en ligne comme le "Robots.txt Tester" de Google Search Console pour vous assurer qu'il fonctionne comme pr√©vu.
6. Surveillez r√©guli√®rement les performances de votre site dans les moteurs de recherche pour vous assurer que les r√®gles d√©finies dans le fichier `robots.txt` n'affectent pas n√©gativement l'indexation de votre site.
7. Mettez √† jour le fichier `robots.txt` si n√©cessaire, en fonction des changements apport√©s √† votre site ou des nouvelles directives des moteurs de recherche.
8. Informez les moteurs de recherche des modifications apport√©es au fichier `robots.txt` en soumettant √† nouveau le fichier via la Google Search Console ou d'autres outils similaires.

## Principales directives

Les deux r√®gles principales se nomment :

**User-agent**: d√©signe le nom d'un robot de moteur de recherche auquel la r√®gle s'applique. Le caract√®re `*` est un joker qui s'applique √† tous les robots.
**Disallow**: d√©signe un r√©pertoire ou une page, relatif au domaine racine, qui ne doit pas √™tre explor√© par le user-agent. Rappelez-vous que, par d√©faut, un robot peut explorer une page ou un r√©pertoire non-bloqu√© par une r√®gle Disallow.

Un fichier `robots.txt` peut-etre compos√© de **directives** qui indiquent aux robots comment interagir avec votre site.
Voici une liste non-exhaustive des  directives :

- `Allow` : Indique les pages ou r√©pertoires que le robot est autoris√© √† visiter (utile pour autoriser des sous-r√©pertoires dans un r√©pertoire interdit).
- `Sitemap` : Indique l'emplacement du fichier sitemap de votre site, ce qui aide les moteurs de recherche √† trouver toutes les pages de votre site.
- `Crawl-delay` : Sp√©cifie le d√©lai (en secondes) entre les requ√™tes du robot pour √©viter de surcharger le serveur.
- `Host` : Indique le nom de domaine pr√©f√©r√© pour le site (utile pour les sites avec plusieurs domaines).
- `Noindex` : Indique aux robots de ne pas indexer une page sp√©cifique (notez que cette directive n'est pas officiellement reconnue par tous les moteurs de recherche).
- `Clean-param` : Utilis√© pour indiquer aux moteurs de recherche de ne pas indexer les URL avec certains param√®tres de requ√™te.
- `Request-rate` : Sp√©cifie le nombre maximum de requ√™tes qu'un robot peut faire dans un intervalle de temps donn√©.
- `Visit-time` : Indique les heures pendant lesquelles un robot est autoris√© √† visiter le site.
- `Disallow: /` : Bloque l'acc√®s √† l'ensemble du site.
- `Allow: /` : Autorise l'acc√®s √† l'ensemble du site.
- `Disallow: /private/` : Bloque l'acc√®s au r√©pertoire `/private/` et √† tout son contenu.
- `Allow: /public/` : Autorise l'acc√®s au r√©pertoire `/public/ ` et √† tout son contenu.
- `Disallow: /temp/` : Bloque l'acc√®s au r√©pertoire `/temp /` et √† tout son contenu.
- `Allow: /public/images/` : Autorise l'acc√®s au r√©pertoire `/public/images/` et √† tout son contenu.
- `Disallow: /admin/` : Bloque l'acc√®s au r√©pertoire `/admin/` et √† tout son contenu.
- `Allow: /public/css/` : Autorise l'acc√®s au r√©pertoire `/public/css/` et √† tout son contenu.
- `Disallow: /cgi-bin/` : Bloque l'acc√®s au r√©pertoire `/cgi-bin/` et √† tout son contenu.
- `Allow: /public/js/` : Autorise l'acc√®s au r√©pertoire `/public/js/` et √† tout son contenu.
- `Disallow: /scripts/` : Bloque l'acc√®s au r√©pertoire `/scripts/` et √† tout son contenu.
- `Allow: /public/fonts/` : Autorise l'acc√®s au r√©pertoire `/public/fonts/` et √† tout son contenu.
- `Disallow: /config/` : Bloque l'acc√®s au r√©pertoire `/config/` et √† tout son contenu.
- `Allow: /public/videos/` : Autorise l'acc√®s au r√©pertoire `/public/videos/` et √† tout son contenu.
- `Disallow: /logs/` : Bloque l'acc√®s au r√©pertoire `/logs/` et √† tout son contenu.
- `Allow: /public/docs/` : Autorise l'acc√®s au r√©pertoire `/public/docs/` et √† tout son contenu.
- `Disallow: /backup/` : Bloque l'acc√®s au r√©pertoire `/backup/` et √† tout son contenu.
- `Allow: /public/api/` : Autorise l'acc√®s au r√©pertoire `/public/api/` et √† tout son contenu.

aller plus loin dans la documentation officielle : [https://developers.google.com/search/docs/advanced/robots/intro?hl=fr](https://developers.google.com/search/docs/advanced/robots/intro?hl=fr)
Un article sur le sujet : [https://www.robotstxt.org/robotstxt.html](https://www.robotstxt.org/robotstxt.html)
Un article sur les robots et WordPress https://wpmarmite.com/robots-txt-wordpress/
Un article de la doc officiellle de WordPress : https://wordpress.org/documentation/article/search-engine-optimization/


#### *Happy coding* ü§ñ
